# -*- coding: utf-8 -*-
"""Keras_image_classification.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mX8XBX1DUUCNhyHzv24GUyArn768rRNx
"""

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/image_classification/Keras_CNN_image_classification

## read_img.py
import os
import cv2
#根据输入的文件夹绝对路径，将该文件夹下的所有指定suffix的文件读取存入一个list,该list的第一个元素是该文件夹的名字
# 폴더의 입력된 절대 경로에 따라 폴더에 지정된 모든 접미사 파일을 읽고 목록에 저장합니다. 목록의 첫 번째 요소는 폴더 이름입니다.
def readAllImg(path,*suffix):
    try:

        s = os.listdir(path)
        resultArray = []
        fileName = os.path.basename(path)
        resultArray.append(fileName)

        for i in s:
            if endwith(i, suffix):
                document = os.path.join(path, i)
                img = cv2.imread(document)
                resultArray.append(img)


    except IOError:
        print ("Error")

    else:
        print ("读取成功")
        return resultArray

#输入一个字符串一个标签，对这个字符串的后续和标签进行匹配
def endwith(s,*endstring):
   resultArray = map(s.endswith,endstring)
   if True in resultArray:
       return True
   else:
       return False

if __name__ == '__main__':

  result = readAllImg("/content/drive/MyDrive/image_classification/Keras_CNN_image_classification/위성사진",'.png')
  print (result[0])
  # cv2.namedWindow("Image")
  # cv2.imshow("Image", result[1])
  # cv2.waitKey(0)
  # cv2.destroyAllWindows()

## read_data
# coding= utf-8
import os
import cv2
import numpy as np

# from read_img import endwith

#输入一个文件路径，对其下的每个文件夹下的图片读取，并对每个文件夹给一个不同的Label
#返回一个img的list,返回一个对应label的list,返回一下有几个文件夹（有几种label)

def read_file(path):
    img_list = []
    label_list = []
    dir_counter = 0


    #对路径下的所有子文件夹中的所有jpg文件进行读取并存入到一个list中
    for child_dir in os.listdir(path):
        child_path = os.path.join(path, child_dir)
         
        for dir_image in os.listdir(child_path):
            print(child_path)
            if endwith(dir_image,'jpg'):
                img = cv2.imread(os.path.join(child_path, dir_image))
                img =cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
                img_list.append(img)
                label_list.append(dir_counter)

        dir_counter += 1

    # 返回的img_list转成了 np.array的格式
    img_list = np.array(img_list)

    return img_list,label_list,dir_counter

#读取训练数据集的文件夹，把他们的名字返回给一个list
def read_name_list(path):
    name_list = []
    for child_dir in os.listdir(path):
        name_list.append(child_dir)
    return name_list

## dataSet.py
# -*- coding: utf-8 -*-

# from read_data import read_file
#from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
# cross_validation to model_selection
from keras.utils import np_utils
import random

#建立一个用于存储和格式化读取训练数据的类
class DataSet(object):
    def __init__(self,path):
        self.num_classes = None
        self.X_train = None
        self.X_test = None
        self.Y_train = None
        self.Y_test = None
        self.extract_data(path)
        #在这个类初始化的过程中读取path下的训练数据

    def extract_data(self,path):
        #根据指定路径读取出图片、标签和类别数
        imgs,labels,counter = read_file(path)
  
        print("输出标记")
        print(labels)

        #将数据集打乱随机分组    
        
        
        X_train,X_test,y_train,y_test = train_test_split(imgs,labels,test_size=0.4,random_state=random.randint(0, 100))
        print("输出训练标记和训练集长度")
        print(y_train)
        print(len(X_train))
        print(X_train[1])
        print("测试长度和测试集标记")
        print(len(X_test))
        print(y_test)
        print("输出和")
        print(counter)

        #重新格式化和标准化
        # 本案例是基于thano的，如果基于tensorflow的backend需要进行修改
        X_train = X_train.reshape(X_train.shape[0], 174, 212, 1)
        X_test = X_test.reshape(X_test.shape[0], 174, 212,1)
        
        
        X_train = X_train.astype('float32')/255
        X_test = X_test.astype('float32')/255
        print(X_train[1])

        #将labels转成 binary class matrices
        Y_train = np_utils.to_categorical(y_train, num_classes=counter)
        Y_test = np_utils.to_categorical(y_test, num_classes=counter)
        
        print(Y_train)
        #将格式化后的数据赋值给类的属性上
        self.X_train = X_train
        self.X_test = X_test
        self.Y_train = Y_train
        self.Y_test = Y_test
        self.num_classes = counter

    def check(self):
        print('num of dim:', self.X_test.ndim)
        print('shape:', self.X_test.shape)
        print('size:', self.X_test.size)

        print('num of dim:', self.X_train.ndim)
        print('shape:', self.X_train.shape)
        print('size:', self.X_train.size)

## train_model.py
# coding= utf-8
# from dataSet import DataSet
from keras.models import Sequential,load_model
from keras.layers import Dense,Activation,Convolution2D,MaxPooling2D,Flatten,Dropout,BatchNormalization
import numpy as np
from keras.callbacks import TensorBoard
# from keras.utils import plot_model
from keras.utils.vis_utils import plot_model
#建立一个基于CNN的识别模型
class Model(object):
    FILE_PATH = "G:/desktop/myProject/model.h5"   #模型进行存储和读取的地方
    

    def __init__(self):
        self.model = None

    #读取实例化后的DataSet类作为进行训练的数据源
    def read_trainData(self,dataset):
        self.dataset = dataset

    #建立一个CNN模型，一层卷积、一层池化、一层卷积、一层池化、抹平之后进行全链接、最后进行分类      其中flatten是将多维输入一维化的函数 dense是全连接层
    def build_model(self):
        self.model = Sequential()
        self.model.add(
            Convolution2D(
                          
                filters=32,
                kernel_size=(5, 5),
                padding='same',
                dim_ordering='tf',
                input_shape=self.dataset.X_train.shape[1:], 
               
            )
        )
        self.model.add( BatchNormalization())

        self.model.add(Activation('relu'))
        self.model.add(
            MaxPooling2D(
                pool_size=(2, 2),
                strides=(2, 2), 
                padding='same'
            )
        )
        

        self.model.add(Convolution2D(filters=64, kernel_size=(5, 5), padding='same'))
        self.model.add(BatchNormalization())
        self.model.add(Activation('relu'))
        self.model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))
        self.model.add(Dropout(0.15))
        
        
        self.model.add(Convolution2D(filters=64, kernel_size=(5, 5), padding='same'))
        self.model.add(BatchNormalization())
        self.model.add(Activation('relu'))
        self.model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))
        self.model.add(Dropout(0.15))
        

        self.model.add(Flatten())
        self.model.add(Dense(512))
        self.model.add(BatchNormalization())
        self.model.add(Activation('relu'))
        self.model.add(Dropout(0.5))
        
        self.model.add(Dense(128))
        self.model.add(BatchNormalization())
        self.model.add(Activation('relu'))
        self.model.add(Dropout(0.5))
        
        self.model.add(Dense(self.dataset.num_classes))
        self.model.add(BatchNormalization())
        self.model.add(Activation('softmax'))
        self.model.summary()
#         plot_model(model,to_file='G:/desktop/myProject/model.png')
        
    #进行模型训练的函数，具体的optimizer、loss可以进行不同选择
    def train_model(self):
        self.model.compile(
            optimizer='adadelta',  #有很多可选的optimizer，例如RMSprop,Adagrad，你也可以试试哪个好，我个人感觉差异不大   adadelta
            loss='squared_hinge',  #你可以选用 categorical_crossentropy  squared_hinge作为loss看看哪个好
            metrics=['accuracy'])

        #epochs、batch_size为可调的参数，epochs为训练多少轮、batch_size为每次训练多少个样本
        self.model.fit(self.dataset.X_train,self.dataset.Y_train,epochs=12,batch_size=20,callbacks=[TensorBoard(log_dir='G:/desktop/myProject/tmp/log')])

    def evaluate_model(self):
        print('\nTesting---------------')
        loss, accuracy = self.model.evaluate(self.dataset.X_test, self.dataset.Y_test)

        print('test loss;', loss)
        print('test accuracy:', accuracy)

    def save(self, file_path=FILE_PATH):
        print('Model Saved.')
        self.model.save(file_path)

    def load(self, file_path=FILE_PATH):
        print('Model Loaded.')
        self.model = load_model(file_path)

    #需要确保输入的img得是灰化之后（channel =1 )且 大小为IMAGE_SIZE的人脸图片
    def predict(self,img):
        img = img.reshape((1,  174, 212,1))
        img = img.astype('float32')
        img = img/255.0
        result = self.model.predict_proba(img)  #测算一下该img属于某个label的概率
        max_index = np.argmax(result) #找出概率最高的

        return max_index,result[0][max_index] #第一个参数为概率最高的label的index,第二个参数为对应概率


if __name__ == '__main__':
    datast = DataSet('/content/drive/MyDrive/image_classification/Keras_CNN_image_classification/위성사진')
    model = Model()
    model.read_trainData(datast)
    model.build_model()
    model.train_model()
    model.evaluate_model()
    model.save()
    #score=model.evaluate()

## test_model.py
# coding= utf-8
from read_data import read_name_list,read_file
from train_model import Model
import cv2
import os 
import numpy as np
from read_img import endwith
from dataSet import DataSet
from keras import backend as K
from keras.utils import np_utils


K.clear_session()
# def test_onePicture(path):
#     model= Model()
#     model.load()
#     img = cv2.imread(path)
#     picType,prob = model.predict(img)
#     if picType != -1:
#         name_list = read_name_list('F:\myProject\pictures\dataset')
#         print(name_list)
#         print( name_list[picType],prob)
#     else:
#         print (" Don't know this person")

# #读取文件夹下子文件夹中所有图片进行识别
# def test_onBatch(path):
#     model= Model()
#     model.load()
#     index = 0
#     img_list, label_list, counter = read_file(path)
#     for i in range(len(img_list)):
#         picType,prob = model.predict(img_list[i])
#         if picType==label_list[i] & picType != -1:
#             index += 1
#     #计算预测正确的概率
#     pro_predict=float(index)/len(img_list)
#     return pro_predict

#读取文件夹下子文件夹中所有图片进行识别
def test_onBatch(path):
    model= Model()
    model.load()
    index = 0
    img_list, label_list, counter = read_file(path)
#     img_list = img_list.reshape(img_list.shape[0], 174, 212, 1)
#     print(img_list.shape[0:])
#     img_list = img_list.astype('float32')/255
#     Label_list = np_utils.to_categorical(label_list, num_classes=counter)
    for img in img_list:
        picType,prob = model.predict(img)
        if picType != -1:
            index += 1
            name_list = read_name_list('G:/desktop/myProject/pictures/test')
            print(name_list)
            print (name_list[picType])
        else:
            print (" Don't know this person")
  
    return index